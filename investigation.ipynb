{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TODOS\n",
    "\n",
    "- check for missing values\n",
    "  - how to fill them in? \n",
    "- check which features to use and if we want to use a reduction method\n",
    "  - [Reduction: PCA vs LDA](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py) (LDA was mentioned in lecture)\n",
    "  - [Get values from LDA](https://stackoverflow.com/questions/13973096/how-do-i-get-the-components-for-lda-in-scikit-learn)\n",
    "  - [LDA step by step](https://machinelearningmastery.com/linear-discriminant-analysis-for-dimensionality-reduction-in-python/)\n",
    "- After that we implement the classifier \n",
    "  - Combine a reduction with the classifier in a [pipeline](https://stackoverflow.com/questions/32860849/classification-pca-and-logistic-regression-using-sklearn) \n",
    "\n",
    "- [tuning pipelines](https://www.kaggle.com/code/mathurutkarsh/pipelines-and-hyperparameter-tuning-in-sklearn)\n",
    "  \n",
    "\n",
    "- To use hyperparameter tuning its best to use our own AMS score as the deciding scorer\n",
    "  - [See here on how to do that](https://scikit-learn.org/stable/modules/model_evaluation.html#defining-your-scoring-strategy-from-metric-functions) \n",
    "  - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import uproot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ROOT;\n",
    "#import lumiere as lm\n",
    "#lm.loadstyle(True);\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "def ams_score(x, y, w, cut):\n",
    "# Calculate Average Mean Significane as defined in ATLAS paper\n",
    "#    -  approximative formula for large statistics with regularisation\n",
    "# x: array of truth values (1 if signal)\n",
    "# y: array of classifier result\n",
    "# w: array of event weights\n",
    "# cut\n",
    "    t = y > cut \n",
    "    s = np.sum((x[t] == 1)*w[t])\n",
    "    b = np.sum((x[t] == 0)*w[t])\n",
    "    return s/np.sqrt(b+10.0)\n",
    "\n",
    "def find_best_ams_score(x, y, w):\n",
    "# find best value of AMS by scanning cut values; \n",
    "# x: array of truth values (1 if signal)\n",
    "# y: array of classifier results\n",
    "# w: array of event weights\n",
    "#  returns \n",
    "#   ntuple of best value of AMS and the corresponding cut value\n",
    "#   list with corresponding pairs (ams, cut) \n",
    "# ----------------------------------------------------------\n",
    "    ymin=min(y) # classifiers may not be in range [0.,1.]\n",
    "    ymax=max(y)\n",
    "    nprobe=200    # number of (equally spaced) scan points to probe classifier \n",
    "    amsvec= [(ams_score(x, y, w, cut), cut) for cut in np.linspace(ymin, ymax, nprobe)] \n",
    "    maxams=sorted(amsvec, key=lambda lst: lst[0] )[-1]\n",
    "    return maxams, amsvec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def printScore(model):\n",
    "\n",
    "    try:\n",
    "        pred_clf = model.predict_proba(x_val)[:, 1]\n",
    "    except:\n",
    "        pred_clf = model.predict(x_val)\n",
    "        pred_clf = pred_clf.reshape((pred_clf.shape[0],))\n",
    "\n",
    "    auc = roc_auc_score(y_val, pred_clf, sample_weight=w_val)\n",
    "    print('AUC:', auc)\n",
    "    bs = find_best_ams_score(y_val, pred_clf, w_val)\n",
    "    print('AMS:', bs[0][0])\n",
    "    print('AMS total:', bs[0][0]*np.sqrt(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-in & to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "input_columns = ['DER_deltaeta_jet_jet', 'DER_deltar_tau_lep', 'DER_lep_eta_centrality', 'DER_mass_MMC', 'DER_mass_jet_jet', \n",
    "                 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_met_phi_centrality', 'DER_prodeta_jet_jet', 'DER_pt_h', \n",
    "                 'DER_pt_ratio_lep_tau', 'DER_pt_tot', 'DER_sum_pt', 'PRI_jet_all_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi', \n",
    "                 'PRI_jet_leading_pt', 'PRI_jet_num', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_subleading_pt', \n",
    "                 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_lep_pt', 'PRI_met', 'PRI_met_phi', 'PRI_met_sumet', 'PRI_tau_eta', 'PRI_tau_phi', \n",
    "                 'PRI_tau_pt', 'transverse_lepton_jet_mass']\n",
    "print(len(input_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDF = ROOT.ROOT.RDataFrame\n",
    "\n",
    "signal_tree_name = 'signal'\n",
    "background_tree_name = 'background'\n",
    "test_tree_name = 'validation'\n",
    "file_name = 'atlas-higgs-challenge-2014-v2_part.root'\n",
    "\n",
    "rdf_signal = RDF(signal_tree_name, file_name)\n",
    "rdf_bkg = RDF(background_tree_name, file_name)\n",
    "rdf_test = RDF(test_tree_name, file_name)\n",
    "\n",
    "reconstruct_transverse_lepton_jet_mass = '''\n",
    "\n",
    "float lep_px = PRI_lep_pt * TMath::Cos(PRI_lep_phi);\n",
    "float lep_py = PRI_lep_pt * TMath::Sin(PRI_lep_phi);\n",
    "float jet_px = PRI_jet_leading_pt * TMath::Cos(PRI_jet_leading_phi);\n",
    "float jet_py = PRI_jet_leading_pt * TMath::Sin(PRI_jet_leading_phi);\n",
    "\n",
    "//calculate angle between jet and lepton\n",
    "float cos_theta = (lep_px*jet_px + lep_py*jet_py) / PRI_lep_pt / PRI_jet_leading_pt;\n",
    "\n",
    "return PRI_lep_pt * PRI_jet_leading_pt * (1 - cos_theta);\n",
    "'''\n",
    "\n",
    "#insertion\n",
    "rdf_signal = rdf_signal.Define('transverse_lepton_jet_mass', reconstruct_transverse_lepton_jet_mass)\n",
    "rdf_bkg = rdf_bkg.Define('transverse_lepton_jet_mass', reconstruct_transverse_lepton_jet_mass)\n",
    "rdf_test = rdf_test.Define('transverse_lepton_jet_mass', reconstruct_transverse_lepton_jet_mass)\n",
    "\n",
    "# label classification to int values\n",
    "rdf_test = rdf_test.Define('IntLabel', '''\n",
    "const char ch = Label[0];\n",
    "const char s = 's';\n",
    "if(ch == s){\n",
    "    return 1;\n",
    "}\n",
    "else{\n",
    "    return 0;\n",
    "}\n",
    "''')\n",
    "\n",
    "\n",
    "df_signal = pd.DataFrame(rdf_signal.AsNumpy())\n",
    "df_bg = pd.DataFrame(rdf_bkg.AsNumpy())\n",
    "df_test = pd.DataFrame(rdf_test.AsNumpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concatination, shuffle and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle;\n",
    "from sklearn.model_selection import train_test_split;\n",
    "\n",
    "#input feature arrays\n",
    "vars_signal = df_signal[input_columns].to_numpy()\n",
    "vars_bg = df_bg[input_columns].to_numpy()\n",
    "vars_test = df_test[input_columns].to_numpy()\n",
    "\n",
    "inputs = np.concatenate([vars_signal, vars_bg])\n",
    "\n",
    "#weights\n",
    "weight_signal = df_signal['Weight'].to_numpy()\n",
    "weight_bg = df_bg['Weight'].to_numpy()\n",
    "weights = np.concatenate([weight_signal, weight_bg])\n",
    "weights = weights.reshape((weights.shape[0],))\n",
    "\n",
    "weights_test = df_test['Weight'].to_numpy()\n",
    "\n",
    "\n",
    "# target classifictionation (1:signal / 0: background)\n",
    "y_signal = np.ones((vars_signal.shape[0], ))\n",
    "y_bg = np.zeros((vars_bg.shape[0], ))\n",
    "\n",
    "targets = np.concatenate([y_signal, y_bg])\n",
    "\n",
    "# for test dataset there is already a classification; convert to int\n",
    "truths_test = df_test.IntLabel.to_numpy()\n",
    "\n",
    "\n",
    "# shuffle \n",
    "inputs, targets, weights = shuffle(inputs, targets, weights)\n",
    "\n",
    "\n",
    "# not for gridcv\n",
    "\n",
    "# training and validation split  (80, 20)\n",
    "x_train, x_val, y_train, y_val, w_train, w_val = train_test_split(inputs, targets, weights, test_size=0.2)\n",
    "#x_train, y_train, = inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler;\n",
    "# \n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(x_train) #set up only on train data\n",
    "# \n",
    "# # tranformation applied to all\n",
    "# x_train = scaler.transform(x_train)\n",
    "# x_val = scaler.transform(x_val)\n",
    "# x_test = scaler.transform(vars_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# \n",
    "# x_train_in = x_train\n",
    "# lda = LinearDiscriminantAnalysis()\n",
    "# lda.fit(x_train, y_train) #define only on training data\n",
    "# \n",
    "# # apply transform to all data\n",
    "# x_train = lda.transform(x_train)\n",
    "# x_val = lda.transform(x_val)\n",
    "# x_test = lda.transform(vars_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# \n",
    "# x_train_pre = x_train\n",
    "# \n",
    "# pca = PCA(n_components=30)\n",
    "# pca.fit(x_train)\n",
    "# \n",
    "# x_train = pca.transform(x_train)\n",
    "# x_val = pca.transform(x_val)\n",
    "# x_test = pca.transform(vars_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# \n",
    "# clf = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1,\n",
    "#     max_depth=8, random_state=0, min_samples_leaf=200).fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom AMS scorer\n",
    "def BuildScorer(validation_x, validation_y, validation_weight):\n",
    "\n",
    "    def AMS_scorer(estimator, X, y):\n",
    "        predictions = estimator.predict_proba(validation_x)[:, 1]\n",
    "        score = find_best_ams_score(validation_y, predictions, validation_weight)\n",
    "        return score[0][0] \n",
    "    \n",
    "    return AMS_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 288 candidates, totalling 576 fits\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=100, clf__n_estimators=100, pca__n_components=20;, score=0.829 total time=  31.8s\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=100, clf__n_estimators=100, pca__n_components=20;, score=0.831 total time=  31.5s\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=100, clf__n_estimators=100, pca__n_components=12;, score=0.829 total time=  19.6s\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=100, clf__n_estimators=100, pca__n_components=12;, score=0.826 total time=  19.4s\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=100, clf__n_estimators=150, pca__n_components=20;, score=0.824 total time=  46.7s\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=100, clf__n_estimators=150, pca__n_components=20;, score=0.827 total time=  46.8s\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=100, clf__n_estimators=150, pca__n_components=12;, score=0.822 total time=  28.8s\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=100, clf__n_estimators=150, pca__n_components=12;, score=0.824 total time=  28.6s\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=100, clf__n_estimators=200, pca__n_components=20;, score=0.827 total time= 1.0min\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=100, clf__n_estimators=200, pca__n_components=20;, score=0.824 total time= 1.0min\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=100, clf__n_estimators=200, pca__n_components=12;, score=0.821 total time=  37.7s\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=100, clf__n_estimators=200, pca__n_components=12;, score=0.819 total time=  37.9s\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=100, clf__n_estimators=400, pca__n_components=20;, score=0.832 total time= 2.0min\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=100, clf__n_estimators=400, pca__n_components=20;, score=0.829 total time= 2.0min\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=100, clf__n_estimators=400, pca__n_components=12;, score=0.824 total time= 1.2min\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=100, clf__n_estimators=400, pca__n_components=12;, score=0.820 total time= 1.2min\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=200, clf__n_estimators=100, pca__n_components=20;, score=0.835 total time=  31.2s\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=200, clf__n_estimators=100, pca__n_components=20;, score=0.832 total time=  30.8s\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=200, clf__n_estimators=100, pca__n_components=12;, score=0.832 total time=  19.1s\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=200, clf__n_estimators=100, pca__n_components=12;, score=0.834 total time=  19.2s\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=200, clf__n_estimators=150, pca__n_components=20;, score=0.831 total time=  45.5s\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=200, clf__n_estimators=150, pca__n_components=20;, score=0.824 total time=  45.5s\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=200, clf__n_estimators=150, pca__n_components=12;, score=0.830 total time=  28.2s\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=200, clf__n_estimators=150, pca__n_components=12;, score=0.827 total time=  28.0s\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=200, clf__n_estimators=200, pca__n_components=20;, score=0.828 total time= 1.0min\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=200, clf__n_estimators=200, pca__n_components=20;, score=0.826 total time= 1.0min\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=200, clf__n_estimators=200, pca__n_components=12;, score=0.827 total time=  37.0s\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=200, clf__n_estimators=200, pca__n_components=12;, score=0.828 total time=  37.0s\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=200, clf__n_estimators=400, pca__n_components=20;, score=0.830 total time= 2.0min\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=200, clf__n_estimators=400, pca__n_components=20;, score=0.829 total time= 2.0min\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=200, clf__n_estimators=400, pca__n_components=12;, score=0.826 total time= 1.2min\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=200, clf__n_estimators=400, pca__n_components=12;, score=0.828 total time= 1.2min\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=300, clf__n_estimators=100, pca__n_components=20;, score=0.833 total time=  30.2s\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=300, clf__n_estimators=100, pca__n_components=20;, score=0.839 total time=  30.4s\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=300, clf__n_estimators=100, pca__n_components=12;, score=0.839 total time=  18.8s\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=300, clf__n_estimators=100, pca__n_components=12;, score=0.836 total time=  18.8s\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=300, clf__n_estimators=150, pca__n_components=20;, score=0.835 total time=  45.0s\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=300, clf__n_estimators=150, pca__n_components=20;, score=0.832 total time=  45.6s\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=300, clf__n_estimators=150, pca__n_components=12;, score=0.837 total time=  27.6s\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=300, clf__n_estimators=150, pca__n_components=12;, score=0.832 total time=  27.5s\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=300, clf__n_estimators=200, pca__n_components=20;, score=0.833 total time=  58.6s\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=300, clf__n_estimators=200, pca__n_components=20;, score=0.832 total time=  58.7s\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=300, clf__n_estimators=200, pca__n_components=12;, score=0.833 total time=  36.1s\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=300, clf__n_estimators=200, pca__n_components=12;, score=0.832 total time=  36.0s\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=300, clf__n_estimators=400, pca__n_components=20;, score=0.835 total time= 2.0min\n",
      "[CV 2/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=300, clf__n_estimators=400, pca__n_components=20;, score=0.831 total time= 2.6min\n",
      "[CV 1/2] END clf__learning_rate=1, clf__max_depth=5, clf__min_samples_leaf=300, clf__n_estimators=400, pca__n_components=12;, score=0.830 total time= 1.5min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler;\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "clf = GradientBoostingClassifier(random_state=0, verbose=0)\n",
    "\n",
    "pipe = Pipeline([ ('scaler', scaler), ('pca', pca), ('clf', clf)])\n",
    "\n",
    "\n",
    "param_grid = {'pca__n_components': [20, 12],\n",
    "                  'clf__n_estimators': [100, 150, 200, 400],\n",
    "                  'clf__min_samples_leaf': [100, 200, 300],\n",
    "                  'clf__max_depth': [5, 8, 10], \n",
    "                  'clf__learning_rate': [1, 0.5, 0.1, 0.05]\n",
    "                }\n",
    "\n",
    "\n",
    "#scoring_function = make_scorer(BuildScorer(w_val), greater_is_better=True)\n",
    "#scoring_function = BuildScorer(x_val, y_val, w_val)\n",
    "#grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring=scoring_function, verbose=4, cv=3, n_jobs=10)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='roc_auc', verbose=4, cv=2)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results = pd.DataFrame(grid_search.cv_results_)\n",
    "grid_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printScore(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### is data missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_signal.isnull().sum().sum() + df_bg.isnull().sum().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
