{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import uproot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ROOT;\n",
    "#import lumiere as lm\n",
    "#lm.loadstyle(True);\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "def ams_score(x, y, w, cut):\n",
    "# Calculate Average Mean Significane as defined in ATLAS paper\n",
    "#    -  approximative formula for large statistics with regularisation\n",
    "# x: array of truth values (1 if signal)\n",
    "# y: array of classifier result\n",
    "# w: array of event weights\n",
    "# cut\n",
    "    t = y > cut \n",
    "    s = np.sum((x[t] == 1)*w[t])\n",
    "    b = np.sum((x[t] == 0)*w[t])\n",
    "    return s/np.sqrt(b+10.0)\n",
    "\n",
    "def find_best_ams_score(x, y, w):\n",
    "# find best value of AMS by scanning cut values; \n",
    "# x: array of truth values (1 if signal)\n",
    "# y: array of classifier results\n",
    "# w: array of event weights\n",
    "#  returns \n",
    "#   ntuple of best value of AMS and the corresponding cut value\n",
    "#   list with corresponding pairs (ams, cut) \n",
    "# ----------------------------------------------------------\n",
    "    ymin=min(y) # classifiers may not be in range [0.,1.]\n",
    "    ymax=max(y)\n",
    "    nprobe=200    # number of (equally spaced) scan points to probe classifier \n",
    "    amsvec= [(ams_score(x, y, w, cut), cut) for cut in np.linspace(ymin, ymax, nprobe)] \n",
    "    maxams=sorted(amsvec, key=lambda lst: lst[0] )[-1]\n",
    "    return maxams, amsvec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def printScore(model):\n",
    "\n",
    "    try:\n",
    "        pred_clf = model.predict_proba(x_val)[:, 1]\n",
    "    except:\n",
    "        pred_clf = model.predict(x_val)\n",
    "        pred_clf = pred_clf.reshape((pred_clf.shape[0],))\n",
    "\n",
    "    auc = roc_auc_score(y_val, pred_clf, sample_weight=w_val)\n",
    "    print('AUC:', auc)\n",
    "    bs = find_best_ams_score(y_val, pred_clf, w_val)\n",
    "    print('AMS:', bs[0][0])\n",
    "    print('AMS total:', bs[0][0]*np.sqrt(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-in & to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "input_columns = ['DER_deltaeta_jet_jet', 'DER_deltar_tau_lep', 'DER_lep_eta_centrality', 'DER_mass_MMC', 'DER_mass_jet_jet', \n",
    "                 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_met_phi_centrality', 'DER_prodeta_jet_jet', 'DER_pt_h', \n",
    "                 'DER_pt_ratio_lep_tau', 'DER_pt_tot', 'DER_sum_pt', 'PRI_jet_all_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi', \n",
    "                 'PRI_jet_leading_pt', 'PRI_jet_num', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_subleading_pt', \n",
    "                 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_lep_pt', 'PRI_met', 'PRI_met_phi', 'PRI_met_sumet', 'PRI_tau_eta', 'PRI_tau_phi', \n",
    "                 'PRI_tau_pt', 'transverse_lepton_jet_mass']\n",
    "print(len(input_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDF = ROOT.ROOT.RDataFrame\n",
    "\n",
    "signal_tree_name = 'signal'\n",
    "background_tree_name = 'background'\n",
    "test_tree_name = 'validation'\n",
    "file_name = 'atlas-higgs-challenge-2014-v2_part.root'\n",
    "\n",
    "rdf_signal = RDF(signal_tree_name, file_name)\n",
    "rdf_bkg = RDF(background_tree_name, file_name)\n",
    "rdf_test = RDF(test_tree_name, file_name)\n",
    "\n",
    "reconstruct_transverse_lepton_jet_mass = '''\n",
    "\n",
    "float lep_px = PRI_lep_pt * TMath::Cos(PRI_lep_phi);\n",
    "float lep_py = PRI_lep_pt * TMath::Sin(PRI_lep_phi);\n",
    "float jet_px = PRI_jet_leading_pt * TMath::Cos(PRI_jet_leading_phi);\n",
    "float jet_py = PRI_jet_leading_pt * TMath::Sin(PRI_jet_leading_phi);\n",
    "\n",
    "//calculate angle between jet and lepton\n",
    "float cos_theta = (lep_px*jet_px + lep_py*jet_py) / PRI_lep_pt / PRI_jet_leading_pt;\n",
    "\n",
    "return PRI_lep_pt * PRI_jet_leading_pt * (1 - cos_theta);\n",
    "'''\n",
    "\n",
    "#insertion\n",
    "rdf_signal = rdf_signal.Define('transverse_lepton_jet_mass', reconstruct_transverse_lepton_jet_mass)\n",
    "rdf_bkg = rdf_bkg.Define('transverse_lepton_jet_mass', reconstruct_transverse_lepton_jet_mass)\n",
    "rdf_test = rdf_test.Define('transverse_lepton_jet_mass', reconstruct_transverse_lepton_jet_mass)\n",
    "\n",
    "# label classification to int values\n",
    "rdf_test = rdf_test.Define('IntLabel', '''\n",
    "const char ch = Label[0];\n",
    "const char s = 's';\n",
    "if(ch == s){\n",
    "    return 1;\n",
    "}\n",
    "else{\n",
    "    return 0;\n",
    "}\n",
    "''')\n",
    "\n",
    "\n",
    "df_signal = pd.DataFrame(rdf_signal.AsNumpy())\n",
    "df_bg = pd.DataFrame(rdf_bkg.AsNumpy())\n",
    "df_test = pd.DataFrame(rdf_test.AsNumpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concatination, shuffle and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle;\n",
    "from sklearn.model_selection import train_test_split;\n",
    "\n",
    "#input feature arrays\n",
    "vars_signal = df_signal[input_columns].to_numpy()\n",
    "vars_bg = df_bg[input_columns].to_numpy()\n",
    "vars_test = df_test[input_columns].to_numpy()\n",
    "\n",
    "inputs = np.concatenate([vars_signal, vars_bg])\n",
    "\n",
    "#weights\n",
    "weight_signal = df_signal['Weight'].to_numpy()\n",
    "weight_bg = df_bg['Weight'].to_numpy()\n",
    "weights = np.concatenate([weight_signal, weight_bg])\n",
    "weights = weights.reshape((weights.shape[0],))\n",
    "\n",
    "weights_test = df_test['Weight'].to_numpy()\n",
    "\n",
    "\n",
    "# target classifictionation (1:signal / 0: background)\n",
    "y_signal = np.ones((vars_signal.shape[0], ))\n",
    "y_bg = np.zeros((vars_bg.shape[0], ))\n",
    "\n",
    "targets = np.concatenate([y_signal, y_bg])\n",
    "\n",
    "# for test dataset there is already a classification; convert to int\n",
    "truths_test = df_test.IntLabel.to_numpy()\n",
    "\n",
    "\n",
    "# shuffle \n",
    "inputs, targets, weights = shuffle(inputs, targets, weights)\n",
    "\n",
    "\n",
    "# not for gridcv\n",
    "\n",
    "# training and validation split  (80, 20)\n",
    "x_train, x_val, y_train, y_val, w_train, w_val = train_test_split(inputs, targets, weights, test_size=0.2)\n",
    "#x_train, y_train = inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom AMS scorer\n",
    "def BuildScorer(validation_x, validation_y, validation_weight):\n",
    "\n",
    "    def AMS_scorer(estimator, X, y):\n",
    "        predictions = estimator.predict_proba(validation_x)[:, 1]\n",
    "        score = find_best_ams_score(validation_y, predictions, validation_weight)\n",
    "        return score[0][0] \n",
    "    \n",
    "    return AMS_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler;\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "clf = GradientBoostingClassifier(random_state=0, verbose=0)\n",
    "\n",
    "pipe = Pipeline([ ('scaler', scaler), ('pca', pca), ('clf', clf)])\n",
    "\n",
    "\n",
    "param_grid = {'pca__n_components': [20, 12],\n",
    "                  'clf__n_estimators': [100, 150, 200, 400],\n",
    "                  'clf__min_samples_leaf': [100, 200, 300],\n",
    "                  'clf__max_depth': [5, 8, 10], \n",
    "                  'clf__learning_rate': [1, 0.5, 0.1, 0.05]\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results = pd.read_csv('halving_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_one = grid_results.sort_values('mean_test_score', ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler;\n",
    " \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train) #set up only on train data\n",
    " \n",
    "# tranformation applied to all\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(vars_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.10/site-packages (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.13.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (1.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from imbalanced-learn) (3.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install imbalanced-learn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE()\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "x_train_pre = x_train\n",
    "\n",
    "pca = PCA(n_components=22)\n",
    "pca.fit(x_train)\n",
    "\n",
    "x_train = pca.transform(x_train)\n",
    "x_val = pca.transform(x_val)\n",
    "x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "#clf = RandomForestClassifier(n_estimators=275, \n",
    "#                            criterion='gini', \n",
    "#                             verbose=1\n",
    "#                           )\n",
    "\n",
    "#clf.fit(x_train, y_train)\n",
    "#\n",
    "#printScore(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## halving grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_iterations: 8\n",
      "n_required_iterations: 12\n",
      "n_possible_iterations: 8\n",
      "min_resources_: 16\n",
      "max_resources_: 52600\n",
      "aggressive_elimination: False\n",
      "factor: 3\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 373248\n",
      "n_resources: 16\n",
      "Fitting 4 folds for each of 373248 candidates, totalling 1492992 fits\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 500, 1000], \n",
    "    'max_features': ['auto', 'sqrt', 'log2'],  \n",
    "    'max_depth': [None, 10, 20, 30, 40, 50], \n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 10], \n",
    "    'bootstrap': [True, False],\n",
    "    'criterion': ['gini', 'entropy', 'log_loss'], \n",
    "    'class_weight': [None, 'balanced', 'balanced_subsample'], \n",
    "    'max_leaf_nodes': [None, 10, 20, 30, 40, 50], \n",
    "    'min_impurity_decrease': [0.0, 0.01, 0.1]\n",
    "    }\n",
    "\n",
    "\n",
    "grid_search = HalvingGridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', verbose=1, cv=4, random_state=0)\n",
    "grid_search.fit(x_train, y_train)\n",
    "grid_results = pd.DataFrame(grid_search.cv_results_)\n",
    "grid_results.to_csv('storage/forest_random_search.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
