{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.28/10\n"
     ]
    }
   ],
   "source": [
    "#import uproot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "import ROOT;\n",
    "#import lumiere as lm\n",
    "#lm.loadstyle(True);\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "def ams_score(x, y, w, cut):\n",
    "# Calculate Average Mean Significane as defined in ATLAS paper\n",
    "#    -  approximative formula for large statistics with regularisation\n",
    "# x: array of truth values (1 if signal)\n",
    "# y: array of classifier result\n",
    "# w: array of event weights\n",
    "# cut\n",
    "    t = y > cut \n",
    "    s = np.sum((x[t] == 1)*w[t])\n",
    "    b = np.sum((x[t] == 0)*w[t])\n",
    "    return s/np.sqrt(b+10.0)\n",
    "\n",
    "def find_best_ams_score(x, y, w):\n",
    "# find best value of AMS by scanning cut values; \n",
    "# x: array of truth values (1 if signal)\n",
    "# y: array of classifier results\n",
    "# w: array of event weights\n",
    "#  returns \n",
    "#   ntuple of best value of AMS and the corresponding cut value\n",
    "#   list with corresponding pairs (ams, cut) \n",
    "# ----------------------------------------------------------\n",
    "    ymin=min(y) # classifiers may not be in range [0.,1.]\n",
    "    ymax=max(y)\n",
    "    nprobe=200    # number of (equally spaced) scan points to probe classifier \n",
    "    amsvec= [(ams_score(x, y, w, cut), cut) for cut in np.linspace(ymin, ymax, nprobe)] \n",
    "    maxams=sorted(amsvec, key=lambda lst: lst[0] )[-1]\n",
    "    return maxams, amsvec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def printScore(model):\n",
    "\n",
    "    try:\n",
    "        pred_clf = model.predict_proba(x_val)[:, 1]\n",
    "    except:\n",
    "        pred_clf = model.predict(x_val)\n",
    "        pred_clf = pred_clf.reshape((pred_clf.shape[0],))\n",
    "\n",
    "    auc = roc_auc_score(y_val, pred_clf, sample_weight=w_val)\n",
    "    print('AUC:', auc)\n",
    "    bs = find_best_ams_score(y_val, pred_clf, w_val)\n",
    "    print('AMS:', bs[0][0])\n",
    "    print('AMS total:', bs[0][0]*np.sqrt(50))\n",
    "\n",
    "\n",
    "\n",
    "def plotLossAccuracy(history):\n",
    "    # Get training and validation loss/accuracy values from history\n",
    "    loss_training = history.history['loss']\n",
    "    loss_validation = history.history['val_loss']\n",
    "    accuracy_training = history.history['accuracy']\n",
    "    accuracy_validation = history.history['val_accuracy']\n",
    "\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(20, 8))\n",
    "    \n",
    "    ax1.plot(loss_training, 'o--', label='training')\n",
    "    ax1.plot(loss_validation, 'o--', label='validation')\n",
    "    ax1.legend(title='loss')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    \n",
    "    ax2.plot(accuracy_training, 'o--', label='training')\n",
    "    ax2.plot(accuracy_validation, 'o--', label='validation')\n",
    "    ax2.legend(title='accuracy')\n",
    "    ax2.set_xlabel('epoch')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plotAMS(history):\n",
    "    # Get training and validation loss/accuracy values from history\n",
    "    ams_training = history.history['ams_metric']\n",
    "    ams_validation = history.history['val_ams_metric']\n",
    "    \n",
    "\n",
    "    fig, ax1 = plt.subplots(1,1, figsize=(10, 8))\n",
    "    \n",
    "    #ax1.plot(ams_training, 'o--', label='training')\n",
    "    ax1.plot(ams_validation, '-', label='validation')\n",
    "    ax1.legend(title='AMS')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-in & to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_columns = ['DER_deltaeta_jet_jet', 'DER_deltar_tau_lep', 'DER_lep_eta_centrality', 'DER_mass_MMC', 'DER_mass_jet_jet', \n",
    "                 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_met_phi_centrality', 'DER_prodeta_jet_jet', 'DER_pt_h', \n",
    "                 'DER_pt_ratio_lep_tau', 'DER_pt_tot', 'DER_sum_pt', 'PRI_jet_all_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi', \n",
    "                 'PRI_jet_leading_pt', 'PRI_jet_num', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_subleading_pt', \n",
    "                 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_lep_pt', 'PRI_met', 'PRI_met_phi', 'PRI_met_sumet', 'PRI_tau_eta', 'PRI_tau_phi', \n",
    "                 'PRI_tau_pt', 'transverse_lepton_jet_mass']\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "input_columns = [ 'transverse_lepton_jet_mass',\n",
    "                'DER_mass_MMC', \n",
    "                'DER_mass_vis', \n",
    "                'DER_mass_transverse_met_lep',\n",
    "                'DER_pt_ratio_lep_tau', \n",
    "                'DER_sum_pt',\n",
    "                'DER_deltar_tau_lep', \n",
    "                'DER_pt_h',\n",
    "                'DER_sum_pt',  \n",
    "                'PRI_met_sumet', \n",
    "                'PRI_tau_pt',\n",
    "                ]\n",
    "\n",
    "'''\n",
    "print(len(input_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDF = ROOT.ROOT.RDataFrame\n",
    "\n",
    "signal_tree_name = 'signal'\n",
    "background_tree_name = 'background'\n",
    "test_tree_name = 'validation'\n",
    "file_name = 'atlas-higgs-challenge-2014-v2_part.root'\n",
    "\n",
    "rdf_signal = RDF(signal_tree_name, file_name)\n",
    "rdf_bkg = RDF(background_tree_name, file_name)\n",
    "rdf_test = RDF(test_tree_name, file_name)\n",
    "\n",
    "reconstruct_transverse_lepton_jet_mass = '''\n",
    "\n",
    "float lep_px = PRI_lep_pt * TMath::Cos(PRI_lep_phi);\n",
    "float lep_py = PRI_lep_pt * TMath::Sin(PRI_lep_phi);\n",
    "float jet_px = PRI_jet_leading_pt * TMath::Cos(PRI_jet_leading_phi);\n",
    "float jet_py = PRI_jet_leading_pt * TMath::Sin(PRI_jet_leading_phi);\n",
    "\n",
    "//calculate angle between jet and lepton\n",
    "float cos_theta = (lep_px*jet_px + lep_py*jet_py) / PRI_lep_pt / PRI_jet_leading_pt;\n",
    "\n",
    "return PRI_lep_pt * PRI_jet_leading_pt * (1 - cos_theta);\n",
    "'''\n",
    "\n",
    "#insertion\n",
    "rdf_signal = rdf_signal.Define('transverse_lepton_jet_mass', reconstruct_transverse_lepton_jet_mass)\n",
    "rdf_bkg = rdf_bkg.Define('transverse_lepton_jet_mass', reconstruct_transverse_lepton_jet_mass)\n",
    "rdf_test = rdf_test.Define('transverse_lepton_jet_mass', reconstruct_transverse_lepton_jet_mass)\n",
    "\n",
    "# label classification to int values\n",
    "rdf_test = rdf_test.Define('IntLabel', '''\n",
    "const char ch = Label[0];\n",
    "const char s = 's';\n",
    "if(ch == s){\n",
    "    return 1;\n",
    "}\n",
    "else{\n",
    "    return 0;\n",
    "}\n",
    "''')\n",
    "\n",
    "\n",
    "df_signal = pd.DataFrame(rdf_signal.AsNumpy())\n",
    "df_bg = pd.DataFrame(rdf_bkg.AsNumpy())\n",
    "df_test = pd.DataFrame(rdf_test.AsNumpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concatination, shuffle and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle;\n",
    "from sklearn.model_selection import train_test_split;\n",
    "\n",
    "#input feature arrays\n",
    "vars_signal = df_signal[input_columns].to_numpy()\n",
    "vars_bg = df_bg[input_columns].to_numpy()\n",
    "vars_test = df_test[input_columns].to_numpy()\n",
    "\n",
    "inputs = np.concatenate([vars_signal, vars_bg])\n",
    "\n",
    "#weights\n",
    "weight_signal = df_signal['Weight'].to_numpy()\n",
    "weight_bg = df_bg['Weight'].to_numpy()\n",
    "weights = np.concatenate([weight_signal, weight_bg])\n",
    "weights = weights.reshape((weights.shape[0],))\n",
    "\n",
    "weights_test = df_test['Weight'].to_numpy()\n",
    "\n",
    "\n",
    "# target classifictionation (1:signal / 0: background)\n",
    "y_signal = np.ones((vars_signal.shape[0], ))\n",
    "y_bg = np.zeros((vars_bg.shape[0], ))\n",
    "\n",
    "targets = np.concatenate([y_signal, y_bg])\n",
    "\n",
    "# for test dataset there is already a classification; convert to int\n",
    "truths_test = df_test.IntLabel.to_numpy()\n",
    "\n",
    "\n",
    "# shuffle \n",
    "inputs, targets, weights = shuffle(inputs, targets, weights)\n",
    "\n",
    "\n",
    "# not for gridcv\n",
    "\n",
    "# training and validation split  (80, 20)\n",
    "x_train, x_val, y_train, y_val, w_train, w_val = train_test_split(inputs, targets, weights, test_size=0.2)\n",
    "#x_train, y_train = inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler;\n",
    " \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train) #set up only on train data\n",
    " \n",
    "# tranformation applied to all\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(vars_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE()\n",
    "x_train, y_train = smote.fit_resample(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#x_train_pre = x_train\n",
    "\n",
    "#pca = PCA(n_components=22)\n",
    "#pca.fit(x_train)\n",
    "\n",
    "#x_train = pca.transform(x_train)\n",
    "#x_val = pca.transform(x_val)\n",
    "#x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "$$\n",
    "AMS = \\sqrt{2((s+b+b_r) \\log(1+\\frac{s}{s+b_r})-s)}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $s, b$: unnormalised true positive and false positive rates, respectively\n",
    "- $b_r = 10$ is the constant regularisation term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-26 11:11:38.614373: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-26 11:11:38.615022: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-26 11:11:38.618786: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-26 11:11:38.662002: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-26 11:11:39.506621: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import Metric\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "\n",
    "def ams_metric(y_true, y_pred):\n",
    "    \n",
    "    s = tf.reduce_sum(y_true * tf.round(y_pred))\n",
    "    b = tf.reduce_sum((1 - y_true) * tf.round(y_pred))\n",
    "    b_r = 10.0\n",
    "    ams = tf.math.sqrt(2 * ((s + b + b_r) * tf.math.log(1 + s / (b + b_r)) - s))\n",
    "    return ams.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training function\n",
    "def train(model, epochs=10, batch_size=128, learning_rate=1e-3, verbose=\"auto\", callbacks=None):\n",
    "    '''\n",
    "    - x: features\n",
    "    - y: labels\n",
    "    - w: sample weights\n",
    "    '''\n",
    "    \n",
    "    # Define loss function, optimizer algorithm and validation metrics\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "        #metrics=['accuracy', AUC]\n",
    "    \n",
    "    # Print summary of the model\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "                         verbose=verbose,  validation_split=0.2, callbacks=callbacks)\n",
    "    #history = model.fit(x_train, y_train, sample_weight=w_train, batch_size=batch_size, epochs=epochs,\n",
    "    #                     verbose=verbose,  validation_split=0.2)\n",
    "        \n",
    "    \n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf;\n",
    "from tensorflow.keras import layers, models, optimizers;\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Define the DNN model\n",
    "def buildModel():\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # input\n",
    "    model.add(layers.Input(shape=(x_train.shape[1],)))\n",
    "    \n",
    "    # hidden\n",
    "    model.add(layers.Dense(2*128, activation='relu', kernel_initializer='glorot_normal'))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(2*128, activation='relu', kernel_initializer='glorot_normal'))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(2*128, activation='relu', kernel_initializer='glorot_normal'))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(128, activation='relu', kernel_initializer='glorot_normal'))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(128, activation='relu', kernel_initializer='glorot_normal'))\n",
    "    model.add(layers.Dropout(0.4))\n",
    "    model.add(layers.Dense(32, activation='relu', kernel_initializer='glorot_normal'))\n",
    "    \n",
    "    # output\n",
    "    model.add(layers.Dense(1, activation='sigmoid', kernel_initializer='glorot_normal'))\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ModelCheckpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_a\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m buildModel()\n\u001b[0;32m----> 5\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\u001b[43mModelCheckpoint\u001b[49m(filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstorgage/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), \n\u001b[1;32m      6\u001b[0m              CSVLogger(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining.log\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      7\u001b[0m             EarlyStopping(patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)] \n\u001b[1;32m      9\u001b[0m history \u001b[38;5;241m=\u001b[39m train(model, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m1024\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-4\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39mcallbacks)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#history = train(model, epochs=400, batch_size=2*1024, learning_rate=2e-3)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ModelCheckpoint' is not defined"
     ]
    }
   ],
   "source": [
    "name = 'model_a'\n",
    "\n",
    "model = buildModel()\n",
    "\n",
    "callbacks = [ModelCheckpoint(filepath=f'storgage/{name}.keras', save_best_only=True), \n",
    "             CSVLogger('training.log'),\n",
    "            EarlyStopping(patience=10)] \n",
    "\n",
    "history = train(model, epochs=800, batch_size=4*1024, learning_rate=2e-4, callbacks=callbacks)\n",
    "#history = train(model, epochs=400, batch_size=2*1024, learning_rate=2e-3)\n",
    "\n",
    "printScore(model)\n",
    "plotLossAccuracy(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('600epoch_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAMS(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(x_val)\n",
    "yhat = yhat.reshape((yhat.shape[0],))\n",
    "\n",
    "ergebnis = find_best_ams_score(y_val, yhat, w_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ergebnis[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ams_metric(y_val, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ops.sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
