{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### TODOS\n",
    "\n",
    "- check for missing values\n",
    "  - how to fill them in? \n",
    "- check which features to use and if we want to use a reduction method\n",
    "  - [Reduction: PCA vs LDA](https://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py) (LDA was mentioned in lecture)\n",
    "  - [Get values from LDA](https://stackoverflow.com/questions/13973096/how-do-i-get-the-components-for-lda-in-scikit-learn)\n",
    "  - [LDA step by step](https://machinelearningmastery.com/linear-discriminant-analysis-for-dimensionality-reduction-in-python/)\n",
    "- After that we implement the classifier \n",
    "  - Combine a reduction with the classifier in a [pipeline](https://stackoverflow.com/questions/32860849/classification-pca-and-logistic-regression-using-sklearn) \n",
    "\n",
    "- [tuning pipelines](https://www.kaggle.com/code/mathurutkarsh/pipelines-and-hyperparameter-tuning-in-sklearn)\n",
    "  \n",
    "\n",
    "- To use hyperparameter tuning its best to use our own AMS score as the deciding scorer\n",
    "  - [See here on how to do that](https://scikit-learn.org/stable/modules/model_evaluation.html#defining-your-scoring-strategy-from-metric-functions) \n",
    "  - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import uproot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import ROOT;\n",
    "#import lumiere as lm\n",
    "#lm.loadstyle(True);\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "def ams_score(x, y, w, cut):\n",
    "# Calculate Average Mean Significane as defined in ATLAS paper\n",
    "#    -  approximative formula for large statistics with regularisation\n",
    "# x: array of truth values (1 if signal)\n",
    "# y: array of classifier result\n",
    "# w: array of event weights\n",
    "# cut\n",
    "    t = y > cut \n",
    "    s = np.sum((x[t] == 1)*w[t])\n",
    "    b = np.sum((x[t] == 0)*w[t])\n",
    "    return s/np.sqrt(b+10.0)\n",
    "\n",
    "def find_best_ams_score(x, y, w):\n",
    "# find best value of AMS by scanning cut values; \n",
    "# x: array of truth values (1 if signal)\n",
    "# y: array of classifier results\n",
    "# w: array of event weights\n",
    "#  returns \n",
    "#   ntuple of best value of AMS and the corresponding cut value\n",
    "#   list with corresponding pairs (ams, cut) \n",
    "# ----------------------------------------------------------\n",
    "    ymin=min(y) # classifiers may not be in range [0.,1.]\n",
    "    ymax=max(y)\n",
    "    nprobe=200    # number of (equally spaced) scan points to probe classifier \n",
    "    amsvec= [(ams_score(x, y, w, cut), cut) for cut in np.linspace(ymin, ymax, nprobe)] \n",
    "    maxams=sorted(amsvec, key=lambda lst: lst[0] )[-1]\n",
    "    return maxams, amsvec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def printScore(model):\n",
    "\n",
    "    try:\n",
    "        pred_clf = model.predict_proba(x_val)[:, 1]\n",
    "    except:\n",
    "        pred_clf = model.predict(x_val)\n",
    "        pred_clf = pred_clf.reshape((pred_clf.shape[0],))\n",
    "\n",
    "    auc = roc_auc_score(y_val, pred_clf, sample_weight=w_val)\n",
    "    print('AUC:', auc)\n",
    "    bs = find_best_ams_score(y_val, pred_clf, w_val)\n",
    "    print('AMS:', bs[0][0])\n",
    "    print('AMS total:', bs[0][0]*np.sqrt(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read-in & to Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "input_columns = ['DER_deltaeta_jet_jet', 'DER_deltar_tau_lep', 'DER_lep_eta_centrality', 'DER_mass_MMC', 'DER_mass_jet_jet', \n",
    "                 'DER_mass_transverse_met_lep', 'DER_mass_vis', 'DER_met_phi_centrality', 'DER_prodeta_jet_jet', 'DER_pt_h', \n",
    "                 'DER_pt_ratio_lep_tau', 'DER_pt_tot', 'DER_sum_pt', 'PRI_jet_all_pt', 'PRI_jet_leading_eta', 'PRI_jet_leading_phi', \n",
    "                 'PRI_jet_leading_pt', 'PRI_jet_num', 'PRI_jet_subleading_eta', 'PRI_jet_subleading_phi', 'PRI_jet_subleading_pt', \n",
    "                 'PRI_lep_eta', 'PRI_lep_phi', 'PRI_lep_pt', 'PRI_met', 'PRI_met_phi', 'PRI_met_sumet', 'PRI_tau_eta', 'PRI_tau_phi', \n",
    "                 'PRI_tau_pt', 'transverse_lepton_jet_mass']\n",
    "print(len(input_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDF = ROOT.ROOT.RDataFrame\n",
    "\n",
    "signal_tree_name = 'signal'\n",
    "background_tree_name = 'background'\n",
    "test_tree_name = 'validation'\n",
    "file_name = 'atlas-higgs-challenge-2014-v2_part.root'\n",
    "\n",
    "rdf_signal = RDF(signal_tree_name, file_name)\n",
    "rdf_bkg = RDF(background_tree_name, file_name)\n",
    "rdf_test = RDF(test_tree_name, file_name)\n",
    "\n",
    "reconstruct_transverse_lepton_jet_mass = '''\n",
    "\n",
    "float lep_px = PRI_lep_pt * TMath::Cos(PRI_lep_phi);\n",
    "float lep_py = PRI_lep_pt * TMath::Sin(PRI_lep_phi);\n",
    "float jet_px = PRI_jet_leading_pt * TMath::Cos(PRI_jet_leading_phi);\n",
    "float jet_py = PRI_jet_leading_pt * TMath::Sin(PRI_jet_leading_phi);\n",
    "\n",
    "//calculate angle between jet and lepton\n",
    "float cos_theta = (lep_px*jet_px + lep_py*jet_py) / PRI_lep_pt / PRI_jet_leading_pt;\n",
    "\n",
    "return PRI_lep_pt * PRI_jet_leading_pt * (1 - cos_theta);\n",
    "'''\n",
    "\n",
    "#insertion\n",
    "rdf_signal = rdf_signal.Define('transverse_lepton_jet_mass', reconstruct_transverse_lepton_jet_mass)\n",
    "rdf_bkg = rdf_bkg.Define('transverse_lepton_jet_mass', reconstruct_transverse_lepton_jet_mass)\n",
    "rdf_test = rdf_test.Define('transverse_lepton_jet_mass', reconstruct_transverse_lepton_jet_mass)\n",
    "\n",
    "# label classification to int values\n",
    "rdf_test = rdf_test.Define('IntLabel', '''\n",
    "const char ch = Label[0];\n",
    "const char s = 's';\n",
    "if(ch == s){\n",
    "    return 1;\n",
    "}\n",
    "else{\n",
    "    return 0;\n",
    "}\n",
    "''')\n",
    "\n",
    "\n",
    "df_signal = pd.DataFrame(rdf_signal.AsNumpy())\n",
    "df_bg = pd.DataFrame(rdf_bkg.AsNumpy())\n",
    "df_test = pd.DataFrame(rdf_test.AsNumpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## concatination, shuffle and split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle;\n",
    "from sklearn.model_selection import train_test_split;\n",
    "\n",
    "#input feature arrays\n",
    "vars_signal = df_signal[input_columns].to_numpy()\n",
    "vars_bg = df_bg[input_columns].to_numpy()\n",
    "vars_test = df_test[input_columns].to_numpy()\n",
    "\n",
    "inputs = np.concatenate([vars_signal, vars_bg])\n",
    "\n",
    "#weights\n",
    "weight_signal = df_signal['Weight'].to_numpy()\n",
    "weight_bg = df_bg['Weight'].to_numpy()\n",
    "weights = np.concatenate([weight_signal, weight_bg])\n",
    "weights = weights.reshape((weights.shape[0],))\n",
    "\n",
    "weights_test = df_test['Weight'].to_numpy()\n",
    "\n",
    "\n",
    "# target classifictionation (1:signal / 0: background)\n",
    "y_signal = np.ones((vars_signal.shape[0], ))\n",
    "y_bg = np.zeros((vars_bg.shape[0], ))\n",
    "\n",
    "targets = np.concatenate([y_signal, y_bg])\n",
    "\n",
    "# for test dataset there is already a classification; convert to int\n",
    "truths_test = df_test.IntLabel.to_numpy()\n",
    "\n",
    "\n",
    "# shuffle \n",
    "inputs, targets, weights = shuffle(inputs, targets, weights)\n",
    "\n",
    "\n",
    "# not for gridcv\n",
    "\n",
    "# training and validation split  (80, 20)\n",
    "x_train, x_val, y_train, y_val, w_train, w_val = train_test_split(inputs, targets, weights, test_size=0.2)\n",
    "#x_train, y_train = inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom AMS scorer\n",
    "def BuildScorer(validation_x, validation_y, validation_weight):\n",
    "\n",
    "    def AMS_scorer(estimator, X, y):\n",
    "        predictions = estimator.predict_proba(validation_x)[:, 1]\n",
    "        score = find_best_ams_score(validation_y, predictions, validation_weight)\n",
    "        return score[0][0] \n",
    "    \n",
    "    return AMS_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler;\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.experimental import enable_halving_search_cv # noqa\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "clf = GradientBoostingClassifier(random_state=0, verbose=0)\n",
    "\n",
    "pipe = Pipeline([ ('scaler', scaler), ('pca', pca), ('clf', clf)])\n",
    "\n",
    "\n",
    "param_grid = {'pca__n_components': [20, 12],\n",
    "                  'clf__n_estimators': [100, 150, 200, 400],\n",
    "                  'clf__min_samples_leaf': [100, 200, 300],\n",
    "                  'clf__max_depth': [5, 8, 10], \n",
    "                  'clf__learning_rate': [1, 0.5, 0.1, 0.05]\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_results = pd.read_csv('halving_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_one = grid_results.sort_values('mean_test_score', ascending=False).head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StandardScaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler;\n",
    " \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train) #set up only on train data\n",
    " \n",
    "# tranformation applied to all\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(vars_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "x_train_pre = x_train\n",
    "\n",
    "pca = PCA(n_components=22)\n",
    "pca.fit(x_train)\n",
    "\n",
    "x_train = pca.transform(x_train)\n",
    "x_val = pca.transform(x_val)\n",
    "x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_search_result = {'clf__learning_rate': 0.025,\n",
    " 'clf__max_depth': 9,\n",
    " 'clf__min_samples_leaf': 75,\n",
    " 'clf__n_estimators': 275,\n",
    " 'pca__n_components': 22}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.9246530124842149\n",
      "AMS: 0.3405259713072378\n",
      "AMS total: 2.4078822348148354\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=275, learning_rate=0.025,\n",
    "     max_depth=9, random_state=0, min_samples_leaf=75).fit(x_train, y_train)\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "printScore(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_pca__n_components</th>\n",
       "      <th>param_clf__n_estimators</th>\n",
       "      <th>param_clf__learning_rate</th>\n",
       "      <th>param_clf__max_depth</th>\n",
       "      <th>param_clf__min_samples_leaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1800</th>\n",
       "      <td>1</td>\n",
       "      <td>0.882692</td>\n",
       "      <td>22</td>\n",
       "      <td>275</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>2</td>\n",
       "      <td>0.882270</td>\n",
       "      <td>22</td>\n",
       "      <td>250</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>3</td>\n",
       "      <td>0.872557</td>\n",
       "      <td>22</td>\n",
       "      <td>275</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>4</td>\n",
       "      <td>0.872307</td>\n",
       "      <td>22</td>\n",
       "      <td>250</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>5</td>\n",
       "      <td>0.872277</td>\n",
       "      <td>22</td>\n",
       "      <td>225</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>6</td>\n",
       "      <td>0.872131</td>\n",
       "      <td>22</td>\n",
       "      <td>250</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>7</td>\n",
       "      <td>0.870643</td>\n",
       "      <td>22</td>\n",
       "      <td>190</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>8</td>\n",
       "      <td>0.860772</td>\n",
       "      <td>22</td>\n",
       "      <td>250</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>9</td>\n",
       "      <td>0.859719</td>\n",
       "      <td>22</td>\n",
       "      <td>225</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>10</td>\n",
       "      <td>0.859588</td>\n",
       "      <td>22</td>\n",
       "      <td>250</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>11</td>\n",
       "      <td>0.859454</td>\n",
       "      <td>22</td>\n",
       "      <td>275</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>12</td>\n",
       "      <td>0.859428</td>\n",
       "      <td>22</td>\n",
       "      <td>190</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>13</td>\n",
       "      <td>0.859381</td>\n",
       "      <td>22</td>\n",
       "      <td>200</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>14</td>\n",
       "      <td>0.859124</td>\n",
       "      <td>22</td>\n",
       "      <td>190</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>15</td>\n",
       "      <td>0.859033</td>\n",
       "      <td>22</td>\n",
       "      <td>200</td>\n",
       "      <td>0.025</td>\n",
       "      <td>9</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rank_test_score  mean_test_score  param_pca__n_components  \\\n",
       "1800                1         0.882692                       22   \n",
       "1799                2         0.882270                       22   \n",
       "1795                3         0.872557                       22   \n",
       "1798                4         0.872307                       22   \n",
       "1797                5         0.872277                       22   \n",
       "1796                6         0.872131                       22   \n",
       "1794                7         0.870643                       22   \n",
       "1788                8         0.860772                       22   \n",
       "1787                9         0.859719                       22   \n",
       "1789               10         0.859588                       22   \n",
       "1785               11         0.859454                       22   \n",
       "1786               12         0.859428                       22   \n",
       "1793               13         0.859381                       22   \n",
       "1792               14         0.859124                       22   \n",
       "1782               15         0.859033                       22   \n",
       "\n",
       "      param_clf__n_estimators  param_clf__learning_rate  param_clf__max_depth  \\\n",
       "1800                      275                     0.025                     9   \n",
       "1799                      250                     0.025                     9   \n",
       "1795                      275                     0.025                     9   \n",
       "1798                      250                     0.025                     9   \n",
       "1797                      225                     0.025                     9   \n",
       "1796                      250                     0.025                     9   \n",
       "1794                      190                     0.025                     9   \n",
       "1788                      250                     0.025                     9   \n",
       "1787                      225                     0.025                     9   \n",
       "1789                      250                     0.025                     9   \n",
       "1785                      275                     0.025                     9   \n",
       "1786                      190                     0.025                     9   \n",
       "1793                      200                     0.025                     9   \n",
       "1792                      190                     0.025                     9   \n",
       "1782                      200                     0.025                     9   \n",
       "\n",
       "      param_clf__min_samples_leaf  \n",
       "1800                           75  \n",
       "1799                           75  \n",
       "1795                           75  \n",
       "1798                           75  \n",
       "1797                           75  \n",
       "1796                           50  \n",
       "1794                           75  \n",
       "1788                           75  \n",
       "1787                           75  \n",
       "1789                           50  \n",
       "1785                           75  \n",
       "1786                           75  \n",
       "1793                           50  \n",
       "1792                           50  \n",
       "1782                           75  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['rank_test_score',\n",
    " 'mean_test_score',\n",
    " 'param_pca__n_components',\n",
    " 'param_clf__n_estimators',\n",
    " 'param_clf__learning_rate',\n",
    " 'param_clf__max_depth',\n",
    " 'param_clf__min_samples_leaf']\n",
    "\n",
    "grid_results = pd.read_csv('second_search.csv')\n",
    "\n",
    "grid_results[cols].sort_values('rank_test_score').head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
